1.
梯度下降法计算整个数据集的梯度，向着梯度反方向移动
学习率大小应设置为合适值，过大会收敛缓慢甚至不收敛，过小会收敛缓慢
目前的一个挑战是局部最小值问题的解决

2.
牛顿法通过Hessian矩阵调整“步幅”，由于需要计算矩阵逆，计算量较大
牛顿法同样面临局部最小值问题，但是可以通过尝试调整学习率解决

3.
随机梯度下降的时间复杂度为O(n)，这是由于随机梯度下降对于每个样本都进行一次梯度运算

4.
动态学习率在开始学习时应该设置较大，加速收敛，有两种衰减方法，分别是指数和多项式衰减
随着迭代次数增加，参数空间靠近最优解（可行解）需要详细搜索，故减小学习率

5.
梯度下降，随机梯度下降，小批量随机梯度下降的区别主要在于每次处理的batch_size不一样